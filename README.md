# SAC:Soft Actor Critic_implementation

This is a **Pytorch implementation of the 'Soft Actor Critic' paper** (Both of the jupyter notebook and python script is used for the 'main' file and for several others too)

Paper: [Soft actor critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor](https://arxiv.org/abs/1801.01290)


Down below are the graphical results on some of the 'mujoco' and 'box2d' environments of this implementation. Various other environments are also to be solved and added and improvements are to be made in the script for better results and to be on par with the SOTA.

### Env: InvertedPendulum-v1 
![](https://github.com/RUFFY-369/SAC_implementation/blob/master/temp/video/inverted_pendulum/openaigym.video.0.29396.video000225.gif) ![](https://github.com/RUFFY-369/SAC_implementation/blob/master/plots/Inverted_pendulum.png)

Note: In the title of the plot there should be '250 scores' instead of '100 scores'

### Env: HalfCheetah-v1 
![](https://github.com/RUFFY-369/SAC_implementation/blob/master/temp/video/half_cheetah/openaigym.video.0.36576.video000975.gif) ![](https://github.com/RUFFY-369/SAC_implementation/blob/master/plots/HalfCheetah-v1.png)

Note: In the title of the plot there should be '1000 scores' instead of '100 scores'

### Env: LunarLanderContinuous-v2 
![](https://github.com/RUFFY-369/SAC_implementation/blob/master/temp/video/Lunar_lander_cont-v2/openaigym.video.0.29980.video000975.gif)![](https://github.com/RUFFY-369/SAC_implementation/blob/master/plots/LunarLanderContinuous-v2.png)





